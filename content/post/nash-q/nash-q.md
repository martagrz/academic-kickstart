---
title: Implementing Nash-Q Learning
date: 2020-02-24
markup: mmark
header-includes:
  - \usepackage[ruled,vlined,linesnumbered]{algorithm2e}
  
---

### Introduction

Implementing single-agent reinforcement learning methods in multi-agent settings where both agents update their values simultaneously is a generally naive method which does not result in convergent policies. Hu and Wellman (2003) attribute this to a neglect of two issues specific to the multi-agent context. Firstly, the environment is no longer stationary since the agents are simultaneously adapting; the result of an agent's choice is going to change based on the choice of the other agents. This means that the theoretical guarantees that support Q-learning no longer apply. If the results of the agent's choice was affected by some stochastic process, then measures could be taken to incorporate the properties in the update rules. However, in this case, non-stationarity is generated by other agents, which leads to the second issue that single-agent techniques ignore: these agents may be presumed rational or at least regular in some informative way. 

Here, we consider the Nash-Q learning algorithm for general-sum Markov games, an extension to the Q-learning algorithm. It was proposed by Hu and Wellman (2003), where, under relatively stringent assumptions, their method guarantees to find an equilibrium. We leave out the theoretical foundations and focus only on the algorithm. You can find the original paper [here](http://www.jmlr.org/papers/v4/hu03a.html) and our implementation [here](https://github.com/martagrz/multi-agent-reinforcement-learning). We assume familiarity with single-agent Q-learning, including the state-value and state-action-value functions. 

### Markov games

A **Markov game** is defined by a tuple $$\left(\mathcal{N}, \mathcal{S}, \{ \mathcal{A^i} \}_{i \in \mathcal{N}}, \mathcal{P}, \{ \mathcal{R}^i \}_{i \in \mathcal{N}}, \gamma \right)$$ where $\mathcal{N} = \{ 1, \dots, N \}$ denotes the set of $N>1$ agents, $\mathcal{S}$ denotes the state space observed by all agents, $\mathcal{A}^i$ denotes the action space of agent $i$. Let $\mathcal{A} := \mathcal{A}^1 \times \dots \times \mathcal{A}^N$ then $\mathcal{P} : \mathcal{S} \times \mathcal{A} \rightarrow \Delta (\mathcal{S})$ denotes the transition probability from any state $s \in \mathcal{S}$ to any state $s' \in \mathcal{S}$ for any joint action $a \in \mathcal{A}$; $R^i: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow \mathbb{R}$ is the reward function that determines the immediate reward received by agent $i$ for a transition from $(s,a)$ to $s'$; $\gamma \in [0,1]$ is the discount factor.

At time $$t$$, each agent $i \in \mathcal{N}$ executes an action $a_t^i$, according to the system state $$s_t$$. The system then transitions to state $s_{t+1}$ and rewards each agent $i$ by $$R^i(s_t,a_t,s_{t+1})$$. The goal of agent $$i$$ is to optimise its long-term reward, by finding the policy $\pi^i : \mathcal{S} \rightarrow \Delta (\mathcal{A}^i)$ such that $a^i_t \sim \pi^i(\cdot|s_t)$. 


### Nash Q-learning algorithm

The learning agent initialises the Q-values with arbitrary values at the beginning of the period. It then chooses an action, observes its own reward and the actions taken by the other agents and their rewards and the new state $s'$. With a learning rate $\alpha$, it then calculates the Nash equilibrium for the state game and updates its Q-values according to the update rule below. The rule is similar to that of single-agent systems, where there is a trade-off between the current Q-value at the state-action pair and the corresponding update based on the observed reward and estimated discounted future rewards from that pair. This trade-off is governed by the learning rate, $\alpha$.  In this case, however, we consider the actions of all players in the state-action pair and the Nash equilibrium value in Nash-Q. 

$$ Q^i_{t+1} (s,a^1,\dots,a^N) = (1-\alpha) \cdot Q^i_t(s,a^1,\dots,a^N) + \alpha \left[R^i_t + \gamma NashQ^i_t(s')\right]  $$

where 

$$NashQ^i_t(s') = \pi^1(s')\cdot \cdot \cdot \pi^N(s') \cdot Q^i_t(s')$$

In order to calculate the Nash equilibrium $$(\pi^1(s')\cdot \cdot \cdot\pi^N(s'))$$, agent $i$ would have to know the other agent's Q-values. But this is not given so agent $i$ must learn about them too. For this, the agent also initialises the other agent's Q-values and updates them according to the same rule as above. This Nash-Q value then acts as the 'belief' over the strategies of the other players. If we assume that rewards and actions are perfectly observable, then this is the true representation of those strategies. In our implementation we have assumed that the learning agent knows that all agents use the $\epsilon$-greedy policy, attributing the $\epsilon$ value across all possible actions and the $1-\epsilon$ value to the greedy action. 

The summary of the algorithm is given below. 

![image.png](/img/posts/nash-q/nash-q-algo.png)

### Implementation 

For the full implementation, see the [github repo](https://github.com/martagrz/multi-agent-reinforcement-learning). In a 4x4 grid, we train two players at starting points (0, 0) and (0, 2) with goals at (3, 2) and (3, 3), respectively. Actions are chosen based on an $\epsilon$-greedy algorithm, with $\epsilon =0.2$, the learning rate is $\alpha=0.2$, the discount rate is $\gamma=0.6$. We train at these values for 1,000 episodes. The quiver plots of the optimal actions at all possible states for both players are shown below.


![Optimal policies](/img/posts/nash-q/optimal_policy_0.png)

We see that neither player crosses the other's path and both arrive at their goal state. This is a very simple example where the agents do not have to interact with each other since the are many paths they can take where they will not cross. We then consider a slightly more interesting example, where the agents have to consider the other's actions. The starting points are now (0,0) and (0,3) and the end goals are (3,3) and (3,0), meaning they both have to make it across the entire grid. 

![Optimal policy](/img/posts/nash-q/optimal_policy_cross_0.png)


#### Implementation with noisy observed reward

It is usually not possible for the agent to perfectly observe the other agents' rewards due to noise. We therefore investigate the effect of including an error in the rewards observed for the other agents on convergence. The algorithms trains at these values for 1,000 episodes and we see that more training is required as the policies do not converge to optimum. If trained for more than 1,000 episodes, then it does converge. Although this contradicts the assumption that the agents have a correct belief over the strategies of the other players, they do have the correct belief *on average*, since the random error is normally distributed around $0$. 

![Optimal policies with noisy observed reward](/img/posts/nash-q/optimal_policy_cross_1.png)

Is convergence affects by the error with which rewards for the other agents is observed? Looking at this quick-and-dirty graph below, which plots the episodes against the number of steps taken until convergence, shows that there does not seem to be much of a difference in steps taken to converge.  

![Optimal policy for player 2](/img/posts/nash-q/n_steps_convergence.png)

### Comments

The algorithm above relies on several restricting assumptions. Firstly, that there exists only one unique Nash equilibrium in every iteration, which even in the grid-world case that we have implemented, is not the case. There are multiple paths that can be taken from the initial state to the goal that all result in the same reward. Moreover, Nash equilibrium relies on the assumption of rationality, meaning that an agent will always take the action that makes them best off. Implementing the $\epsilon$-greedy to encourage exploration in our implementation does undermine this assumption somewhat, but still it can still be considered rational as to maximise the reward over the entire episode. 

The idea of bounded rationality argues that choices made by agents are restricted to the amount of information or cognitive ability they possess, hence they will not always choose the action that makes them best off. This is much more compelling when applying algorithms such as these in the real world, as individuals may not always be aware of the best option for them, for example, when there are too many items on the menu and you are not able to look through them in the amount of time you have before you need to order. 

Lastly, Nash equilibrium assumes that the agent holds the correct belief over the strategies of other players. In this algorithm, this is addressed by the Q-tables that an agent holds for all other players. This is computationally feasible since we have only considered two agents, but as the number of agents grows, the space complexity (i.e. how much you hold in memory) increases exponentially. In the way it has been implemented, the joint Nash policy can ignore the rules of the game, as the Nash-Q update considers only the probability distribution over the actions of the other agents. It may be the case that the most optimal action for both players results in them being in the same cell, which is not allowed in the game. Moreover, the Nash-Q update rule relies on observability of the actions and rewards of the other agents. In practice, actions and rewards may be observed with a lag, partially or not at all. In which case, other approximations or notions of equilibrium would need to be considered to enable the agent to form a `correct' belief over the actions of the other players. 

Further developments on this algorithm include deep Nash-Q learning (Casgrain et al, 2019), correlated equilibrium learning (Greenwald et al, 2003), batch RL learning to find approximate Nash equilibrium (Maillard et al, 2010). 

It is worth noting that this method considers only symmetric games, meaning that the players are interchangeable and they face the same reward function. If this were not the case, the Q-table update for the other agents would become more difficult as they do not have the same states and actions. In this case, the problem can be forced or decomposed to be symmetric by introducing an equal probability for each agent to have a role. More on asymmetric games in a future post... 



### References

* Hu, J. and Wellman, M.P., 2003. Nash Q-learning for general-sum stochastic games. *Journal of machine learning research, 4* (Nov), pp.1039-1069.
* Zhang, K., Yang, Z. and Ba≈üar, T., 2019. Multi-Agent Reinforcement Learning: A Selective Overview of Theories and Algorithms. *arXiv preprint arXiv:1911.10635.*
* Casgrain, P., Ning, B. and Jaimungal, S., 2019. Deep Q-learning for Nash equilibria: Nash-DQN. *arXiv preprint arXiv:1904.10554.*
* Greenwald, A., Hall, K. and Serrano, R., 2003, August. Correlated Q-learning. In *ICML* (Vol. 20, No. 1, p. 242).
* Maillard, O.A., Munos, R., Lazaric, A. and Ghavamzadeh, M., 2010, October. Finite-sample analysis of Bellman residual minimization. In *Proceedings of 2nd Asian Conference on Machine Learning* (pp. 299-314).


